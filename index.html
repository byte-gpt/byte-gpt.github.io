<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Beyond Language Models: Byte Models are Digital World Simulators</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Beyond Language Models: Byte Models are Digital World Simulators</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="mailto:shangda@mail.ccom.edu.cn" target="_blank">Shangda Wu</a><sup>1,2 ‚ôØ</sup></span>
            <span class="author-block">
              <a href="mailto:xuta@microsoft.com" target="_blank">Xu Tan</a><sup>1 ‚ôØ</sup></span>
            <span class="author-block">
              <a href="mailto:ziliwang.do@gmail.com" target="_blank">Zili Wang</a><sup>3</sup></span>
            </span>
            <span class="author-block">
              <a href="mailto:ruiwa@microsoft.com" target="_blank">Rui Wang</a><sup>1</sup></span>
            </span>
            <span class="author-block">
              <a href="mailto:lxiaobing@ccom.edu.cn" target="_blank">Xiaobing Li</a><sup>2</sup></span>
            </span>
            <span class="author-block">
              <a href="mailto:sms@tsinghua.edu.cn" target="_blank">Maosong Sun</a><sup>2,4 ùÑ™</sup></span>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <br>
            <span class="author-block"><sup>1</sup>Microsoft Research Asia<br><sup>2</sup>Central Conservatory of Music, China<br><sup>3</sup>Independent Researcher<br><sup>4</sup>Tsinghua University, China</span>
            <br>
            <span class="eql-cntrb"><small><br><sup>‚ôØ</sup>Equal Contribution</small></span>
            <span class="corresp-auth"><small><br><sup>ùÑ™</sup>Corresponding author</small></span>
          </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2402.19155" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Hugging Face model link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/sander-wood/bgpt/tree/main" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <span class="icon">
                          <img src="https://cdn-lfs.huggingface.co/repos/96/a2/96a2c8468c1546e660ac2609e49404b8588fcf5a748761fa72c154b2836b4c83/942cad1ccda905ac5a659dfd2d78b344fccfb84a8a3ac3721e08f488205638a0?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27hf-logo.svg%3B+filename%3D%22hf-logo.svg%22%3B&response-content-type=image%2Fsvg%2Bxml&Expires=1709504340&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwOTUwNDM0MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy85Ni9hMi85NmEyYzg0NjhjMTU0NmU2NjBhYzI2MDllNDk0MDRiODU4OGZjZjVhNzQ4NzYxZmE3MmMxNTRiMjgzNmI0YzgzLzk0MmNhZDFjY2RhOTA1YWM1YTY1OWRmZDJkNzhiMzQ0ZmNjZmI4NGE4YTNhYzM3MjFlMDhmNDg4MjA1NjM4YTA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=WciGum7D-AMfxEC47XB84v85XAJ5ZuL5Y6AxAeqvDPTFDVVy7fDupl1ju0kQi18AkrMwORZz8lA4q88zPCk-GyFbt4RVFLRJzkK7lWAGDgy9cJyEbSOSbKY10PyqowLcPC44f2d07eNcqF3N%7Eb0aBJ7GJcPskWEnZqH1IcsFigmq5Gcc5NNyPLI1BMC70uBmpBgfzdR8B4r%7EeJBw4nHSqz80mFqh4-wicbTMFF23fr9kDKJRNxIqqS46fy5IF%7EdNlqOQdoF%7Ei6l2ZBso-rygxQf6MzUthOWMyRcdoh5sQOHzIjbe6pJt%7Eq9jlXwCAXzEVgYvtC0ASD1XJQbniD8YjQ__&Key-Pair-Id=KVTP0A1DKRTAX" alt="Hugging Face Icon">
                        </span>
                        
                      </span>
                      <span>Model</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/sanderwood/bgpt" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Traditional deep learning often overlooks bytes, the basic units of the digital world, where all forms of information and operations are encoded and manipulated in binary format. Inspired by the success of next token prediction in natural language processing, we introduce bGPT, a model with next byte prediction to simulate the digital world. bGPT matches specialized models in performance across various modalities, including text, audio, and images, and offers new possibilities for predicting, simulating, and diagnosing algorithm or hardware behaviour. It has almost flawlessly replicated the process of converting symbolic music data, achieving a low error rate of 0.0011 bits per byte in converting ABC notation to MIDI format. In addition, bGPT demonstrates exceptional capabilities in simulating CPU behaviour, with an accuracy exceeding 99.99% in executing various operations. Leveraging next byte prediction, models like bGPT can directly learn from vast binary data, effectively simulating the intricate patterns of the digital world.
          </p>
         </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/tasks.png" alt="The bGPT framework" style="width: 95%; height: auto;" class="center">
        <h6 class="subtitle has-text-left">
          The bGPT framework simulates digital systems through native binary data, and integrates diverse data types into a single model, treating everything as a byte sequence. This approach simplifies integration and expands application possibilities in the digital world.
        </h6>
      </div>
      <div class="item" style="text-align: center;">
        <!-- Your image here -->
        <img src="static/images/model.png" alt="The bGPT model" style="width: 50%; height: auto;" class="center">
        <h6 class="subtitle has-text-left">
          bGPT segments byte sequences into patches, predicts next patch features with a patch-level decoder, and reconstructs bytes within patches using these features with a byte-level decoder.
        </h6>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Brief introduction -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Bytes are the foundation of all digital data, devices, and software, from computer processors to operating systems in everyday electronics. Therefore, training models for next byte prediction can potentially lead to a paradigm shift in deep learning, allowing them to truly understand and simulate all activities in the digital world. This has practical benefits not only in conventional areas, but also in some underexplored areas such as boosting cybersecurity, improving computer diagnostics, optimizing data compression, and even advancing complex tasks like reverse-engineering the source code of that software from its binary representation.
            <br><br>
            In this paper, we introduce bGPT, a model designed for binary data processing and digital world modelling by next byte prediction. The digital world includes not only digital media files, traditionally the focus of deep learning models, but also extends to the intricate realm of digital systems, ranging from hardware architectures to complex algorithms. bGPT transcends traditional deep learning boundaries by directly interpreting and manipulating binary data, enabling a more intrinsic and holistic understanding of the digital world. Its advantages are two-fold: 1) <strong>Interpreting Digital System</strong>: By training on byte sequences, bGPT can learn the patterns of digital systems, enabling it to predict, simulate, and diagnose algorithm or hardware behaviour. This ability allows for the reconstruction of complex systems from binary data. 2) <strong>Unified Modelling</strong>: bGPT integrates various data types into a single framework, treating everything as a byte sequence. This simplifies modelling and allows for easy integration of various data sources.
            <br><br>
            Our experiments include two main areas: 1) well-studied tasks like generative modelling and classification on digital media data (e.g., text, audio, and images); and 2) relatively underexplored tasks intrinsic to binary-native operations, including data conversion and CPU state modelling, which represent algorithm and hardware simulation, respectively. The demo page sequentially showcases models pre-trained on <a href="https://huggingface.co/datasets/sander-wood/irishman"><font color="#3273DC">IrishMAN</font></a> for data conversion, <a href="https://github.com/sanderwood/bgpt"><font color="#3273DC">CPU states</font></a> for CPU state modelling, <a href="https://huggingface.co/datasets/wikipedia"><font color="#3273DC">Wikipedia</font></a> for text, <a href="https://www.image-net.org/"><font color="#3273DC">ImageNet</font></a> for images, and <a href="https://www.openslr.org/12"><font color="#3273DC">LibriSpeech</font></a> for audio. All showcased generative samples from bGPT are produced using the same data preprocessing, model architecture, hyperparameters, and training objectives, without any modality-specific customizations.
          </p>
         </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End brief introduction -->

<!-- Format conversion -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <br>
      <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Data Conversion</h2>
      </div>
      <br>
      <div class="content has-text-justified">
        <p>
          This process involves converting data from one format to another, with symbolic music formats such as ABC notation and MIDI files serving as our main examples. For background information on ABC notation and MIDI, please refer to Appendix A in <a href="https://arxiv.org/abs/2402.19155">our paper</a>. In this task, bGPT employs the generative modelling approach on concatenated byte sequences of paired ABC and MIDI files, separated by a special patch. The bGPT model learns to convert text-based ABC notation music scores into binary MIDI performance signals and, reversely, convert MIDI back into ABC notation. This necessitates the ability to simulate and reverse-engineer the <a href="https://github.com/xlvector/abcmidi">conversion algorithm</a>, which indicates an essential capability for modelling the digital world.</p>

          In our data conversion tasks with bGPT, we found that the model typically achieves highly accurate conversions between ABC notation and MIDI files, closely mirroring the ground truth. Despite occasional decoding issues with converted MIDI files, bGPT demonstrates the remarkable capability to correct source data errors or more compactly encode conversions without losing accuracy. Below, we present a set of examples that illustrate these capabilities, including bGPT-converted MIDI from ABC notation, ABC notation from MIDI, and the original MIDI and ABC notation data for reference. For ease of demonstration, we've rendered each piece of data into MP3 audio and PDF sheet music. You can download the original data pack <a href="static/samples/conversion-samples.zip">here</a>.
          <br>
        </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="conversion-item1" style="text-align: center;">
          <audio controls>
            <source src="static/audio/conversion/converted-1422.abc.mid.mp3" type="audio/mp3">
          </audio>
          <br>
          <br>
          <iframe src="static/pdfs/converted-1422.abc.mid.pdf" width="100%" height="550"></iframe>
        </div>
        <div class="conversion-item2" style="text-align: center;">
          <audio controls>
            <source src="static/audio/conversion/original-1422.mid.mp3" type="audio/mp3">
          </audio>
          <br>
          <br>
          <iframe src="static/pdfs/original-1422.mid.pdf" width="100%" height="550"></iframe>
        </div>
        <div class="conversion-item3" style="text-align: center;">
          <audio controls>
            <source src="static/audio/conversion/converted-1422.mid.abc.mid.mp3" type="audio/mp3">
          </audio>
          <br>
          <br>
          <iframe src="static/pdfs/converted-1422.mid.abc.xml.pdf" width="100%" height="550"></iframe>
        </div>
        <div class="conversion-item4" style="text-align: center;">
          <audio controls>
            <source src="static/audio/conversion/original-1422.abc.mid.mp3" type="audio/mp3">
          </audio>
          <br>
          <br>
          <iframe src="static/pdfs/original-1422.abc.xml.pdf" width="100%" height="550"></iframe>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End data conversion -->


<!-- CPU state modelling -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <br>
      <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">CPU State Modelling</h2>
      </div>
      <br>
      <div class="content has-text-justified">
        <p>
          The model is fed with concatenated sequences of low-level machine instructions followed by a series of CPU register states. The objective is to accurately predict how the state updates with each instruction until the program halts. This task demonstrates the capacity of bGPT to interpret operational data and replicate digital activities within hardware.
          <br><br>
          For CPU state modelling, we introduce the <a href="https://github.com/sanderwood/bgpt"><font color="#3273DC">CPU states</font></a> dataset (with 2.1 million instances), offering a simplified representation of CPU behaviour for ease of data collection and evaluation. Each dataset instance contains a 1KB memory block with varying numbers of machine instructions, followed by a sequence of 16-byte CPU register states. These states include various instructions, totaling 21 unique types with 43 variants, such as data movement, logical operations, and arithmetic operations. Within each state, 1 byte each is allocated for the Program Counter (PC) and Accumulator (ACC), 4 bytes are allocated for the Instruction Register (IR), with an additional 10 bytes reserved for general-purpose registers. Instances are generated by executing random sequences of 1 to 256 instructions and capturing the state after each execution. Despite simplifications, this dataset effectively simulates typical CPU behaviour. See Appendix B in <a href="https://arxiv.org/abs/2402.19155"><font color="#3273DC">our paper</font></a> for more details.
        </div>
        <br>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <iframe  src="static/pdfs/cpu.pdf" width="100%" height="550">
          </iframe>
          
        <h6 class="subtitle has-text-left">
          <br>
          In this example, bGPT flawlessly executed all 251 consecutive instructions, achieving a perfect performance in modelling CPU states by predicting the next state from the current state and an instruction. For clarity, we translate byte sequences into a readable format, with the original binary file accessible <a href="/static/samples/cpu.bin"><font color="#3273DC">here</font></a>.
        </h6>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End CPU state modelling -->


<!-- Text generation -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <br>
      <div class="columns is-centered has-text-centered">
        <h2 class="title">Text Generation</h2>
      </div>
      <br>
      <div class="content has-text-justified">
        <p>
          bGPT diverges from traditional subword encoding by adopting byte-level text encoding, which requires no vocabulary and thus supports any language. This method, akin to character-level encoding but potentially longer for non-European languages due to multi-byte characters, increases computational demands. To mitigate this, bGPT utilizes a hierarchical Transformer architecture capable of producing texts up to 8KB in size, significantly surpassing the typical 3KB to 4KB output of GPT-2 (where each token occupies 3-4 bytes). Pre-trained on <a href="https://huggingface.co/datasets/wikipedia"><font color="#3273DC">Wikipedia</font></a>, bGPT is capable of generating texts that exhibit a stylistic and thematic fidelity comparable to GPT-2. However, it may also produce logical inconsistencies or inaccuracies, limitations commonly found in models of the 110M scale, especially in extended texts. Furthermore, it occasionally struggles with non-English terms, a consequence of its primary training on English Wikipedia content. Despite these limitations, bGPT exhibits fewer degeneration cases (i.e., repetition), compared to GPT-2. Below, we present a set of examples that illustrate these capabilities, including optimal and poor examples generated by bGPT, as well as GPT2-generated and real Wikipedia articles for reference.
        </p>
      <br>
      <div id="results-carousel" class="carousel results-carousel">
        <!-- PDF 1 and its caption -->
        <div class="pdf-item">
          <iframe src="static/pdfs/good.pdf" width="100%" height="550"></iframe>
        </div>
        
        <!-- PDF 2 and its caption -->
        <div class="pdf-item">
          <iframe src="static/pdfs/bad.pdf" width="100%" height="550"></iframe>
        </div>
        
        <!-- PDF 3 and its caption -->
        <div class="pdf-item">
          <iframe src="static/pdfs/gpt.pdf" width="100%" height="550"></iframe>
        </div>
        
        <!-- PDF 4 and its caption -->
        <div class="pdf-item">
          <iframe src="static/pdfs/real.pdf" width="100%" height="550"></iframe>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End text generation -->


<!-- Image generation -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <br>
      <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Image Generation</h2>
      </div>
      <br>
      <div class="content has-text-justified">
        <p>
          bGPT is capable of generating images by predicting the next byte in a sequence of bytes that represent an image. The model is pre-trained on the <a href="https://www.image-net.org/"><font color="#3273DC">ImageNet</font></a> dataset, and it generates images with a resolution of 32x32 pixels. Due to the sequential processing nature of byte-level encoding, bGPT struggles to capture the essential two-dimensional spatial relationships within images <a href="https://arxiv.org/pdf/2305.07185.pdf"><font color="#3273DC">(Yu et al., 2023)</font></a> at the current scale. This results in images with noticeable artifacts and noise, as well as a lack of coherent structure. Despite this, simply scaling the model size while retaining this sequential processing approach could still hold promise for achieving state-of-the-art results <a href="https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf"><font color="#3273DC">(Chen et al., 2020)</font></a>. Below, we present a set of examples that illustrate the capabilities and limitations of bGPT, including examples generated by bGPT, as well as real images from the ImageNet dataset for reference.
        </p>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item" style="text-align: center;">
        <!-- Your image here -->
        <img src="static/images/collage-generated.bmp" alt="The bGPT framework" style="width: 30%; height: auto;" class="center">
        <h6 class="subtitle has-text-centered">
          <br>
          A collage of images generated by bGPT pre-trained on the ImageNet dataset.
          <br>
          The texture and lighting effects are generally accurate, but identifying main subjects in these generated images is challenging.
        </h6>
      </div>
      <div class="item" style="text-align: center;">
        <!-- Your image here -->
        <img src="static/images/collage-real.bmp" alt="The bGPT model" style="width: 30%; height: auto;" class="center">
        <h6 class="subtitle has-text-centered">
          <br>
          A collage of randomly selected real images from the ImageNet dataset.
        </h6>
      </div>
  </div>
</div>
  </div>
</section>
<!-- End image generation -->

<!-- Audio generation -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <br>
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Audio Generation</h2>
      </div>
      <div class="content has-text-justified">
        <p>
          By modelling audio data as a sequence of bytes, bGPT can generate audio samples with a duration of 1 second with a sampling rate of 8000 Hz. The model is pre-trained on <a href="https://www.openslr.org/12">LibriSpeech</a>,  segmented into 1-second clips, making the 1-second duration <a href="https://huggingface.co/datasets/speech_commands">Speech Commands v2</a> ideal for fine-tuning and showcasing. The generated audio samples are of varying quality, with some examples being nearly indistinguishable from real audio, while others contain noticeable artifacts and noise. Below, we present a set of examples that illustrate the capabilities and limitations of bGPT, including good and poor examples generated by bGPT, as well as real audio samples from the Speech Commands v2 dataset for reference.
        </p>
      </div>
      <br>
      <!-- Page 1 -->
      <div class="columns is-multiline is-centered has-text-centered">
        <div class="column is-full">
          <audio controls>
            <source src="static/audio/good/follow.wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/good/no.wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/good/zero.wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/good/one.wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/good/two.wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/good/seven.wav" type="audio/wav">
          </audio>
        </div>
      </div>
      <h6 class="subtitle has-text-centered">
        Good examples generated by bGPT fine-tuned on Speech Commands v2 dataset.
        <br>
        With samples of "follow", "no", "zero", "one", "two", and "seven".
        <br>
        <br>
      </h6>
      <!-- Page 2 -->
      <div class="columns is-multiline is-centered has-text-centered">
        <div class="column is-full">
          <audio controls>
            <source src="static/audio/real/follow_515-1.wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/real/no_1771-1.wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/real/zero_4880-1.wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/real/one_2658-1.wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/real/two_3985-1.wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/real/seven_3074-1.wav" type="audio/wav">
          </audio>
        </div>
      </div>
      <h6 class="subtitle has-text-centered">
        Randomly selected real examples from the Speech Commands v2 dataset.
        <br>
        With samples of "follow", "no", "zero", "one", "two", and "seven".
        <br>
        <br>
      </h6>
      <!-- Page 3 -->
      <div class="columns is-multiline is-centered has-text-centered">
        <div class="column is-full">
          <audio controls>
            <source src="static/audio/bad/bad (1).wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/bad/bad (2).wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/bad/bad (3).wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/bad/bad (4).wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/bad/bad (5).wav" type="audio/wav">
          </audio>
          <audio controls>
            <source src="static/audio/bad/bad (6).wav" type="audio/wav">
          </audio>
        </div>
      </div>
      <h6 class="subtitle has-text-centered">
        Poor examples generated by bGPT fine-tuned on Speech Commands v2 dataset.
        <br>
        These examples contain noticeable artifacts and noise.
        <br>
        <strong><i>Note: Please reduce the volume before playing these poor examples!</i></strong>
        <br>
        <br>
      </h6>
    </div>
  </div>
</section>
<!-- End audio generation -->


<!-- Paper discussion -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Discussion</h2>
        <div class="content has-text-justified">
          <p>
            The bGPT model demonstrates its strength in the versatility and adaptability of byte models, capable of processing a wide range of data types, including traditional media formats. This capability marks a significant departure from the limitations associated with conventional deep learning models, which are typically confined to specific formats and tasks. By directly operating on native binary data, bGPT facilitates the modelling of algorithm or hardware behaviours, offering a unique advantage. 
            <br><br>
            Nonetheless, our experiments illuminate opportunities for improvement. In this study, we confine the modelling to short audio segments and low-resolution images, a consequence of the resource-intensive nature intrinsic to byte models. Due to limited computational resources, we only investigated data conversion between ABC notation and MIDI, without broader assessments across alternate formats. Furthermore, to simplify data collection and evaluation, our CPU state modelling experiments focused solely on simplified CPUs, omitting the use of real modern CPUs, which are considerably more complex.
            <br><br>
            Future research directions for byte models include: 1) reducing computational cost to make training byte models more feasible; 2) scaling models and dataset sizes to accommodate a broader range of native binary data, as well as handling larger digital media files such as high-resolution images and videos; and 3) improving model performance, particularly for underexplored tasks involving native binary data across diverse application domains.
          </p>
         </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End discussion -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{wu2024language,
        title={Beyond Language Models: Byte Models are Digital World Simulators}, 
        author={Shangda Wu and Xu Tan and Zili Wang and Rui Wang and Xiaobing Li and Maosong Sun},
        year={2024},
        eprint={2402.19155},
        archivePrefix={arXiv},
        primaryClass={cs.LG}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
